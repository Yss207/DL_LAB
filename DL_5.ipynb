{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d38d782-fddf-4f47-b019-9dcf02c3b19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Continuous Bag of Words (CBOW) Model - Hybrid Version\n",
    "# ============================================\n",
    "# 1️⃣ Import Libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "366ce0d2-d25b-492f-9d1e-2b9bb90cd36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Corpus: ['we', 'are', 'about', 'to', 'study', 'the', 'idea', 'of', 'a', 'computational', 'process', 'computational', 'processes', 'are', 'abstract', 'beings', 'that', 'inhabit', 'computers', 'as']\n",
      "Total words: 50\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 2️⃣ Data Preparation\n",
    "# -------------------------\n",
    "# A slightly bigger corpus (like your advanced file)\n",
    "text = \"\"\"We are about to study the idea of a computational process.\n",
    "Computational processes are abstract beings that inhabit computers.\n",
    "As they evolve, processes manipulate other abstract things called data.\n",
    "The evolution of a process is directed by a pattern of rules called a program.\n",
    "People create programs to direct processes.\"\"\"\n",
    "\n",
    "# Basic text cleaning\n",
    "text = re.sub('[^A-Za-z]+', ' ', text)   # remove special characters\n",
    "text = text.lower()                      # lowercase\n",
    "corpus = text.split()                    # split into words\n",
    "\n",
    "print(\"Sample Corpus:\", corpus[:20])\n",
    "print(\"Total words:\", len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6171b41f-de3e-460c-adf5-a3f134f72142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 36\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary\n",
    "vocab = set(corpus)\n",
    "vocab_size = len(vocab)\n",
    "word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx2word = {i: word for word, i in word2idx.items()}\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "140b6d84-4fab-4f81-add2-dd4b88f80836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Training Data:\n",
      "['we', 'are', 'to', 'study'] -> about\n",
      "['are', 'about', 'study', 'the'] -> to\n",
      "['about', 'to', 'the', 'idea'] -> study\n",
      "['to', 'study', 'idea', 'of'] -> the\n",
      "['study', 'the', 'of', 'a'] -> idea\n",
      "\n",
      "Total (context, target) pairs: 46\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 3️⃣ Generate Training Data\n",
    "# -------------------------\n",
    "\n",
    "data = []\n",
    "for i in range(2, len(corpus) - 2):\n",
    "    context = [corpus[i - 2], corpus[i - 1], corpus[i + 1], corpus[i + 2]]\n",
    "    target = corpus[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "print(\"\\nSample Training Data:\")\n",
    "for c, t in data[:5]:\n",
    "    print(f\"{c} -> {t}\")\n",
    "\n",
    "print(\"\\nTotal (context, target) pairs:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fed7e12-0f93-43e6-b04c-f23fb41faed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 4️⃣ Model Setup (CBOW)\n",
    "# -------------------------\n",
    "def one_hot_encoding(word):\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    one_hot[word2idx[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "embedding_dim = 10\n",
    "W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "W2 = np.random.rand(embedding_dim, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce1161c-0bde-4041-8049-71f47cacead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/1000, Loss: 24.7859\n",
      "Epoch 400/1000, Loss: 4.2255\n",
      "Epoch 600/1000, Loss: 1.8380\n",
      "Epoch 800/1000, Loss: 1.1076\n",
      "Epoch 1000/1000, Loss: 0.7731\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5️⃣ Training the Model\n",
    "# -------------------------\n",
    "def cbow_forward(context_words):\n",
    "    hidden = np.mean([W1[word2idx[w]] for w in context_words], axis=0)\n",
    "    output = np.dot(W2.T, hidden)\n",
    "    prediction = softmax(output)\n",
    "    return prediction, hidden\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for context, target in data:\n",
    "        y_pred, h = cbow_forward(context)\n",
    "        target_one_hot = one_hot_encoding(target)\n",
    "        error = y_pred - target_one_hot\n",
    "\n",
    "        # Gradients\n",
    "        dW2 = np.outer(h, error)\n",
    "        dW1 = np.zeros_like(W1)\n",
    "        for w in context:\n",
    "            dW1[word2idx[w]] += np.dot(W2, error)\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= lr * dW1\n",
    "        W2 -= lr * dW2\n",
    "\n",
    "        loss += -np.sum(target_one_hot * np.log(y_pred + 1e-9))\n",
    "\n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f49beb-afe4-45cc-94a6-989b1cee7476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sample Predictions ===\n",
      "Context: ['is', 'directed', 'a', 'pattern'] | Predicted: by | Actual: by\n",
      "Context: ['of', 'a', 'process', 'computational'] | Predicted: computational | Actual: computational\n",
      "Context: ['the', 'idea', 'a', 'computational'] | Predicted: of | Actual: of\n",
      "Context: ['idea', 'of', 'computational', 'process'] | Predicted: a | Actual: a\n",
      "Context: ['program', 'people', 'programs', 'to'] | Predicted: create | Actual: create\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 6️⃣ Output Predictions\n",
    "# -------------------------\n",
    "print(\"\\n=== Sample Predictions ===\")\n",
    "for context, target in random.sample(data, 5):\n",
    "    prediction, _ = cbow_forward(context)\n",
    "    predicted_word = idx2word[np.argmax(prediction)]\n",
    "    print(f\"Context: {context} | Predicted: {predicted_word} | Actual: {target}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
